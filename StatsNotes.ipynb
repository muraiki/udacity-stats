{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "------\n",
      "Basics\n",
      "------\n",
      "\n",
      "Construct: Cannot be measured in a clear quantitative way.\n",
      "Operational Definition: How a construct will be measured. Ex: measuring happiness by # of smiles per day.\n",
      "\n",
      "Standard Normal Distribution: A normal distribution with mean 0 and sd 1; the distribution of all z-scores of a population\n",
      "\n",
      "Standard Deviation for Population:\n",
      "sqrt(sum(squares of deviations) / n)\n",
      "\n",
      "Standard Deviation for Sample:\n",
      "sqrt(sum(squares of deviations) / (n - 1))\n",
      "\n",
      "Symbols:\n",
      "mu = mean of population\n",
      "sigma = standard deviation\n",
      "xbar = mean of sample\n",
      "\n",
      "------------------------\n",
      "Sampling and Probability\n",
      "------------------------\n",
      "\n",
      "68% of samples fall within 1 sd of the mean\n",
      "95% of samples fall within 2 sd of the mean\n",
      "\n",
      "Distribution of Sample Means:\n",
      "A distribution that represents all possible samples for a given sample size n.\n",
      "This distribution is always normal.\n",
      "Its mean is approx the same as its parent.\n",
      "\n",
      "Standard Error: Standard deviation of the distribution of sample means\n",
      "= sigma (of population) / sqrt(n)\n",
      "\n",
      "Z-score: For a distribution of sample means, this value corresponds to how many standard deviations a sample mean falls from mu.\n",
      "The z-score indicates the area underneath the curve of the distribution of sample means,\n",
      "which corresponds to the percent chance that a randomly selected sample mean will fall into that area.\n",
      "The mean has a z-score of 0.\n",
      "= (xbar - mu) / se\n",
      "\n",
      "Standardize: To plot samples using z-scores; allows us to compare samples from different distributions\n",
      "\n",
      "----------\n",
      "Estimation\n",
      "----------\n",
      "\n",
      "Treatment: A change applied to a sample (such as trying a new teaching method for a sample of 20 vs the population of 100)\n",
      "\n",
      "Intervention: The application of a treatment\n",
      "\n",
      "Critical Value: Z-score for a given percantage interval on the distribution of sample means\n",
      "95% corresponds to 1.96\n",
      "\n",
      "Point estimate: The mean (0 z-score) of a given sample size; xbar\n",
      "\n",
      "Confidence Interval: If an intervention is applied, this is the range wherein the mean of the population will fall.\n",
      "Calculated using the desired confidance interval's critical value, cv\n",
      "(mean - (se * cv)), (mean + (se * cv))\n",
      "\n",
      "Margin of error: Half of the range of the confidence interval\n",
      "\n",
      "------------------\n",
      "Hypothesis Testing\n",
      "------------------\n",
      "\n",
      "Critical Regions: Indicates sections of the distribution of sample means and their likelihood\n",
      "\n",
      "One-tailed test: Only looks at critical region on one side of the distribution\n",
      "Directional (treatment will either increase or decrease population parameter)\n",
      "May miss if the treatment has opposite effect.\n",
      "Used when comparing one treatment to another existing; only care if it is more effective.\n",
      "\n",
      "Two-tailed test: Looks at critical regions on both sides of the distribution\n",
      "Non-directional (treatment will change population parameter in any way)\n",
      "More conservative; detects if treatment has opposite effect.\n",
      "\n",
      "- One-tailed Unlikely critical regions -\n",
      "| Alpha levels | Z-Score |\n",
      "|--------------|---------|\n",
      "| a = .05      | 1.65    |\n",
      "| a = .01      | 2.32    |\n",
      "| a = .001     | 3.08    |\n",
      "\n",
      "- Two-tailed Unlikely critical regions -\n",
      "| Alpha levels | Z-Score |\n",
      "|--------------|---------|\n",
      "| a = .05      | +/-1.96 |\n",
      "| a = .01      | +/-2.57 |\n",
      "| a = .001     | +/-3.27 |\n",
      "\n",
      "Null hypothesis (H sub n): No change in population parameters after some treatment\n",
      "Doesn't fall in critical region for given alpha level.\n",
      "\n",
      "Alternative hypothesis (H sub a or H sub 1): Change in population parameters after some treatment\n",
      "Falls in critical region for given alpha level.\n",
      "\n",
      "Type I Error: Null hypothesis is true but we reject it\n",
      "Type II Error: Null hypothesis is false but we accept it\n",
      "\n",
      "-------\n",
      "t-Tests\n",
      "-------\n",
      "\n",
      "t-Distribution: distribution plotted by difference in means / sample standard deviation [ S = sqrt(sum of squared deviations / n - 1) ]\n",
      "More prone to error since it depends on S instead of sigma.\n",
      "Shape is more spread out and less tall.\n",
      "As n increases, the sample standard deviation S approaches sigma (population sd) and the distribution becomes more normal, with skinnier tails.\n",
      "\n",
      "T test standard error is called \"Standard Error of the Mean\" or SEM\n",
      "    S [of sample] / sqrt(n)\n",
      "    The SD quantifies scatter \u2014 how much the values vary from one another.\n",
      "    The SEM quantifies how precisely you know the true mean of the population. It takes into account both the value of the SD and the sample size.\n",
      "    http://www.graphpad.com/guides/prism/6/statistics/index.htm?stat_semandsdnotsame.htm\n",
      "\n",
      "Degrees of Freedom: How many choices can be made until a forced choice must be made.\n",
      "Or, how many pieces of information are available to estimate another piece of information.\n",
      "\n",
      "x_1 + x_2 + ... + x_n = xbar * n\n",
      "When picking samples, n - 1 samples can be anything, but the last sample chosen must make it so that the sum of samples equals xbar * n.\n",
      "As such, there are n - 1 degrees of freedom.\n",
      "\n",
      "Effective sample size: n - 1\n",
      "Only n - 1 values are independant after we know the mean of a sample.\n",
      "\n",
      "One sample t-Test:\n",
      "t = (xbar - mu_0) / (s / sqrt n)\n",
      "Where mu_0 is the mean of the population the sample was drawn from\n",
      "s is the sample standard deviation (in contrast to sigma when calculating the standard error,\n",
      "since sigma is unknown)\n",
      "xbar - mu_0 is the difference between the sample mean (the point estimate) and mu_0\n",
      "s / sqrt n measures the difference between the sample mean and mu_0 that we would expect by chance\n",
      "\n",
      "P-value: Probability of getting a particular t statistic if the null hypothesis is true\n",
      "P-value one-tailed test:\n",
      "Value (area) above the t statistic for positive t statistic\n",
      "Value (area) below the t statistic for negative t statistic\n",
      "P-value two-tailed test: Both areas\n",
      "\n",
      "Reject the null: Reject the null hypothesis when the P-value is less than the alpha level.\n",
      "\n",
      "Cohen's d: standardized mean difference that measures the distance between means in standardized units\n",
      "    (xbar - mu) / sd  when comparing sample to population\n",
      "    (xbar1 - xbar2) / pooled_standard_deviation  when comparing two samples in an independent t test\n",
      "Measures how far the sample's mean is from the populations, in terms of standard deviations.\n",
      "Helps us to understand the _magnitude_ of a difference, since even a minor difference will become statistically\n",
      "significant with a large enough sample size. http://forums.udacity.com/questions/100049890/cohens-d?page=1&focusedAnswerId=100050200#100050200\n",
      "\"Cohen suggested that d=0.2 be considered a 'small' effect size, 0.5\n",
      "represents a 'medium' effect size and 0.8 a 'large' effect size. This\n",
      "means that if two groups' means don't differ by 0.2 standard\n",
      "deviations or more, the difference is trivial, even if it is\n",
      "statistically signficant.\"\n",
      "\n",
      "Subtract one normal dist from another:\n",
      "    xbar_d = xbar_2 - xbar_1\n",
      "    sd = sqrt(s1**2 - s2**2)\n",
      "\n",
      "Dependent T-Test\n",
      "Same subject takes the test twice\n",
      "    \"Within subject design\"\n",
      "        Two conditions: One patient with two treatments, or one patient with a control and a treatment\n",
      "        Pre/post test\n",
      "        Growth over time (logitudinal study)\n",
      "Two columns of data, x_i, y_i, with d_i = absolute difference between x and y\n",
      "\n",
      "Two conditions:\n",
      "t = (xbar_subD - 0) / (s / sqrt(n))\n",
      "Where xbar_subD is the difference between the null and alternative hypotheses.\n",
      "We are trying to see how different the alt hypo is from zero.\n",
      "\n",
      "Types of Effect Size Measures\n",
      "Difference of means\n",
      "    Good when using numbers whose meanings are clear, like #/hrs slept as opposed to score on personality test\n",
      "    Cohen's D - measures difference of means in standard deviations\n",
      "Correlation Measures\n",
      "    R^2 - Proportion (%) of variation in one variable that is related to (\"explained by\") another variable\n",
      "        aka \"coefficient of determination\"\n",
      "r^2 = t^2 / (t^2 + degrees of freedom) :t is val from t-test, not critical value\n",
      "\n",
      "-----\n",
      "Presenting Statistics\n",
      "-----\n",
      "\n",
      "Descriptive Statistics\n",
      "    Mean and SD\n",
      "Inferential Statistics\n",
      "    Hypothesis test\n",
      "        What kind of test? ex: 1 sample t-test\n",
      "        Actual value of test statistic\n",
      "        degrees of freedom\n",
      "        p-value\n",
      "        direction of test\n",
      "        alpha level\n",
      "\n",
      "T-test APA style presentation\n",
      "    t(df) = X.XX, p = .XX, direction\n",
      "\n",
      "Presenting Confidence Intervals\n",
      "    Confidence level, eg 95%\n",
      "    Lower limit\n",
      "    Upper limit\n",
      "    CI on what? (on a single mean, diff between two means, etc)\n",
      "\n",
      "CI APA style\n",
      "    Confidence interval on the mean difference; 95% CI = (4,6)\n",
      "\n",
      "Presenting Effect Size Measures\n",
      "    d, r^2, etc\n",
      "\n",
      "Effect Size APA style\n",
      "    d = x.xx\n",
      "    r^2 = .xx\n",
      "\n",
      "----\n",
      "T-Tests: Independent Samples\n",
      "----\n",
      "\n",
      "First, for dependent samples / within-subject design:\n",
      "    Advantages: Use fewer subjects, cost effective, less time consuming\n",
      "    Disadvantages\n",
      "        Carry-over effects: eg: Testing new math teaching method, students will necessarily be better 2nd time\n",
      "        Order might influence results\n",
      "\n",
      "Independent Samples\n",
      "    between-subject design\n",
      "\n",
      "Mean changes\n",
      "mu = mu_1 - mu_2\n",
      "\n",
      "T-statistic changes because it is based on two sample sizes and two sds\n",
      "Population: sqrt(variance_1^2 + variance_2^2)\n",
      "Sample: sqrt(SD_1^2 + SD_2^2)\n",
      "\n",
      "Standard error changes\n",
      "sqrt( (SD_1^2 / n_1) + (SD_2^2 / n_2) )\n",
      "\n",
      "Degrees of freedom changes\n",
      "n_1 + n_2 - 2\n",
      "or [n_1 - 1] + [n_2 - 1]\n",
      "\n",
      "Pooled Variance = S_p^2\n",
      "(SS1 + SS2) / (df1 + df2)\n",
      "    where SS = sum of squares\n",
      "\n",
      "Using Pooled Variance in Standard Error for T Test\n",
      "sqrt( (PV_x^2 / n_x) + (PV_y^2 / n_y )\n",
      "\n",
      "T statistic when H_0 (null hypothesis) is != 0\n",
      "For instance, if the null hypothesis is that mu_x - mu_y = 10:\n",
      "((xbar - ybar) - 10) / se :[calculate se using pooled variance]\n",
      "\n",
      "Independent T Test Assumptions:\n",
      "samples should be random, populations should be approximately normal (less important when n>30)\n",
      "sample data can estimate population variances\n",
      "population variances are roughly equal\n",
      "\n",
      "-------\n",
      "One way ANOVA\n",
      "-------\n",
      "\n",
      "ANOVA - analaysis of variance\n",
      "One way ANOVA has one independent variable\n",
      "\n",
      "ANOVA provides a way to compare multiple samples without conducting multiple T-Tests\n",
      "5 Samples compared two ways would require 5! / (2 * (5 - 2)!) tests  formula: n! / (x * (n - x)!)\n",
      "\n",
      "F statistic (or ratio) = between-group variability / within-group variability\n",
      "\n",
      "F = n * \u2211(xbar_k - xbar_g)^2 / (k - 1) = SSbetween / DOFbetween = Mean Square between\n",
      "    ----------------------------------   ----------------------   -------------------\n",
      "        \u2211(x_i - xbar_k)^2 / dof          SSwithin / DOFwithin     Mean Square within\n",
      "where n = sample size (not total # of samples; if each sample has 4 items, n =4)\n",
      "where dof = N - k [N total number of values from all samples, k is number of samples]\n",
      "\n",
      "Total degrees of freedom DOF_total = N-1\n",
      "    since (k - 1) + N - k == N-1\n",
      "\n",
      "Total Variation = SS_between + SS_within = \u2211(x_i - xbar_g)^2\n",
      "\n",
      "ANOVA: distance or variability between means / error\n",
      "For the numerator, find the average squared deviation of each sample mean from the total mean\n",
      "\n",
      "Grand mean: xbar sub G - The toal mean, or mean of sample means\n",
      "When sample sizes are equal, grand mean == mean of all values\n",
      "When sample sizes unequal, mean of all values will have larger samples overrepresented; grand mean != mean of all values\n",
      "\n",
      "Between-group Variability (between sample means): Variability increases as the distance between sample means increases\n",
      "\n",
      "Within-group Variability (between samples): As sample variability increases, the less likely population means with differ significantly\n",
      "\n",
      "Small ANOVA statistic: Accept null; within subject variability is large relative to between subject variability. None of the means are significantly different from each other.\n",
      "\n",
      "Large ANOVA statistic: Reject null; between subject variability is large relative to within subject variability. Some means are significantly different from each other.\n",
      "    Must conduct followup tests called \"multiple comparison tests\" to determine which mean(s) is/are significantly different\n",
      "\n",
      "---------\n",
      "Multiple Comparison Tests\n",
      "---------\n",
      "\n",
      "Used in ANOVA after finding F statistic to compare all means with each other.\n",
      "\n",
      "Tukey's HSD (Honestly Sigificant Difference)\n",
      "    Makes pairwise comparisons between means\n",
      "    q * sqrt(MSwithin / n)\n",
      "    MSwithin is the pulled variance or average squared deviation\n",
      "        therefore sqrt(MSwithin) = pooled standard deviation\n",
      "    n = size of one sample eg: in 4 groups of 5 n=5\n",
      "    q = Studentized Range Statistic; adjusts HSD so less likely to commit Type I Error\n",
      "        Adjusts upwards as n increases\n",
      "        Look up in q table with dof = total degrees of freedom; eg: 5 items in 4 groups =  16 dof\n",
      "            k = how many sample groups; eg: 5 items in 4 groups of samples, k = 5\n",
      "    dof is dof for a whole sample eg: in 4 groups of 5 dof = 16\n",
      "\n",
      "If any 2 samples have a difference > HSD, two samples are honestly significantly different\n",
      "\n",
      "Cohen's D: Must compute one for each pair of samples\n",
      "d = (xbar1 - xbar2) / sqrt(MSwithin)\n",
      "\n",
      "n^2 (\"eta squared\") = similar to r^2 but for ANOVA. Proprotion of total variation that is due to between-group differences (explained variation).\n",
      "Use _sum of squares_ not mean square\n",
      "n^2 = SSbetween / SStotal\n",
      "    > .14 is considered big\n",
      "\n",
      "Reporting results of ANOVA\n",
      "F(2,6) = 27  p < 0.05 n^2 0.90\n",
      "\n",
      "------\n",
      "ANOVA with differently sized groups\n",
      "------\n",
      "Grand Mean = sum of all samples / total # of samples\n",
      "\n",
      "SSbetween = \u2211n_k(xbar_k - xbar_g)^2\n",
      "eg: drugSSbetween = sum([len(each) * (np.mean(each) - drugGrandMean)**2 for each in [placebo, drug1, drug2, drug3]])\n",
      "\n",
      "------\n",
      "ANOVA assumptions\n",
      "------\n",
      "Normal -> Can violate if sample size is large\n",
      "Homogeneity of variance -> Can violate if nearly equal sample sizes and ratio of any two variances <= 4\n",
      "\n",
      "\n",
      "------\n",
      "Correlation\n",
      "------\n",
      "Used for understanding relationships between multiple variables.\n",
      "\n",
      "For two variables, use x and y\n",
      "    x = predictor / explanatory / independent variable\n",
      "    y = outcome / response / dependent variable\n",
      "\n",
      "Often plotted on a scatterplot\n",
      "\n",
      "Relationship: A strong relationship betweeen x and y has certain qualities\n",
      "    Direction: How the y variable changes as x increases (positive or negative)\n",
      "    \n",
      "A strong relationship indicates high correlation\n",
      "\n",
      "Correlation coefficient: r (or Pearson's r)\n",
      "    seen previously in t tests\n",
      "    r = cov(x, y) / (sigma_x * sigma_y)\n",
      "        where cov = covariance function; how much they vary together\n",
      "        sigma_x * sigma_y descirbes how they vary _apart_ from each other\n",
      "    r is not written as %, but r^2 shows % of variation in y explained by variation in x\n",
      "        r^2 is also called the Coefficient of Determination\n",
      "    r measures strength of relationship by how closely data falls on a straight line\n",
      "    requires that each dataset be normally distributed\n",
      "    r IS SENSITIVE TO OUTLIERS\n",
      "\n",
      "p (rho) is the true correlation for the population\n",
      "    H_null is rho = 0; H_a rho < 0 or rho > 0 or rho != 0\n",
      "\n",
      "Hypothesis testing for Pearson's\n",
      "    t = (r * sqrt(N - 2)) / sqrt(1 - r**2)\n",
      "    df = N - 2\n",
      "        where N - 2 is used because of two groups, X and Y. If there are a different number, adjust N accordingly\n",
      "\n",
      "Correlation vs Causation\n",
      "    X <- A -> Y : A is mediating variable\n",
      "    X -> A -> Y : A is mediating variable\n",
      "    Ambiguous Temperal Precedence  eg: drug use causing delinquency vs delinquency causing drug use\n",
      "    Third Variable Problem  eg: Students who score high SAT have high grades in college; increasing SAT = increase college grades\n",
      "        Could be third variable such as motivation that influence both SAT and grades\n",
      "    Post Hoc Fallacy: Some people who commit violent crimes played violent video games before; video games caused violence.\n",
      "\n",
      "-----\n",
      "Regression\n",
      "-----\n",
      "\n",
      "Regression line / Line of best fit\n",
      "    Helps describe the data by helping to visualize relationship between x and y\n",
      "    Helps make predictions\n",
      "\n",
      "Observed (y) vs Expected (\u0177) values\n",
      "    The expected value is what the regression line predicts; observed is actual value\n",
      "    Residual: difference between y and \u0177\n",
      "\n",
      "Regression coefficients\n",
      "    The slope and y-intercept of \u0177\n",
      "    For Udacity class: a = y-intercept and b is slope\n",
      "\n",
      "Calculating the regression line\n",
      "    y = mx + b\n",
      "    minimizes sum of squared residuals\n",
      "    slope = (\u2211(x_i - xbar) * (y_i - ybar)) / \u2211(x_i -xbar)^2\n",
      "    slope also = pearson's r * (sigma_y / sigma_x) \n",
      "    y-intercept is (xbar, ybar) since regression line will necessarily pass through it\n",
      "\n",
      "Calculating standard error of the estimate\n",
      "    \u2211(y-\u0177)^2 / N-2\n",
      "\n",
      "Hypothesis Testing for Slope\n",
      "    The same as hypothesis testing for R, because both tests ask the same general question: Are the two variables linearly related?\n",
      "    Beta_1 = populate slope; Beta_naught = population y-intercept\n",
      "    dof = N - 2  where N is number of data points (x,y pairs)\n",
      "\n",
      "Simple linear regression weakness: outliers\n",
      "    Seriously modify the regression line\n",
      "\n",
      "Regression line full equation (including a / y-intercept)\n",
      "yhat = r * (sigma_y / sigma_x) * x + ybar - r * (sigma_y / sigma_x) * xbar\n",
      "\n",
      "Multiple regression\n",
      "    yhat = b * x_1 + c * x_2 ... + a\n",
      "    Tell us how y changes with each change in each respective variable\n",
      "    R = multiple correlation coefficient; one outcome or response variable and more than one predictor -- not pearson's\n",
      "        strength of y and combined set of predictors\n",
      "    R**2 = proportion of variability in y explained by combined set of predictors\n",
      "    Make sure to check if each predictor is statistically significant or not! It could be that only one or none are.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}